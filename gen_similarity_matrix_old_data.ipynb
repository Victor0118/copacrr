{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd \n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model =  KeyedVectors.load_word2vec_format('/Castor-data/embeddings/word2vec/GoogleNews-vectors-negative300.txt', binary=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'is',\n",
       " 'a',\n",
       " 'cooool',\n",
       " '#dummysmiley',\n",
       " ':',\n",
       " ':-)',\n",
       " ':-P',\n",
       " '<3',\n",
       " 'and',\n",
       " 'some',\n",
       " 'arrows',\n",
       " '<',\n",
       " '>',\n",
       " '->',\n",
       " '<--']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "tknzr = TweetTokenizer()\n",
    "s0 = \"This is a cooool #dummysmiley: :-) :-P <3 and some arrows < > -> <--\"\n",
    "tknzr.tokenize(s0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cos_sim(a, b):\n",
    "    return dot(a, b)/(norm(a)*norm(b))\n",
    "\n",
    "def calculate_cosine_similarity_matrix(query, document):\n",
    "    query_terms = tknzr.tokenize(query)\n",
    "    doc_terms = tknzr.tokenize(document)\n",
    "    \n",
    "    sim_mat = []\n",
    "    for query_term in query_terms:\n",
    "        row = []\n",
    "        for doc_term in doc_terms:\n",
    "            if doc_term not in model:\n",
    "                doc_term = \"UNK\"\n",
    "            if query_term not in model:\n",
    "                query_term = \"UNK\"\n",
    "            \n",
    "            sim_vector = cos_sim(model[query_term], model[doc_term])\n",
    "            row.append(sim_vector)\n",
    "        sim_mat.append(row)\n",
    "        \n",
    "    # print(np.array(sim_mat))\n",
    "    return sim_mat\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "46037it [00:42, 1071.10it/s]\n",
      "54986it [00:47, 1167.68it/s]\n",
      "60000it [00:51, 1154.78it/s]\n",
      "55000it [00:57, 955.66it/s] \n"
     ]
    }
   ],
   "source": [
    "#  Generate from the new ql results\n",
    "# for year in range(2011, 2015):\n",
    "#     f = open(\"/u4/w85yang/MatchZoo/data/tweets/TweetCorpus/test_ql_{}.txt\".format(year)) \n",
    "#     for l in tqdm(f):\n",
    "#         sim, a, b, qid, docid = l.replace(\"\\n\", \"\").split(\"\\t\")\n",
    "#         directory = \"/mnt/collections/w85yang/cosine/topic_doc_mat/{}\".format(qid)\n",
    "#         out = os.path.join(directory, \"{}.npy\".format(docid))\n",
    "#         if not os.path.exists(directory):\n",
    "#             os.makedirs(directory)\n",
    "#         sim_mat = calculate_cosine_similarity_matrix(a, b)\n",
    "#         np.save(out, sim_mat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 27s, sys: 8.32 s, total: 2min 36s\n",
      "Wall time: 2min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for year in [\"train_2011\", \"test_2011\", \"train_2013\", \"test_2013\"]:\n",
    "    folder = \"/u4/w85yang/deep-tweet-search/data/twitter/order_by_rel/{}\".format(year)\n",
    "    fa = open(os.path.join(folder, \"a.toks\")) \n",
    "    fb = open(os.path.join(folder, \"b.toks\")) \n",
    "    fsim = open(os.path.join(folder, \"sim.txt\")) \n",
    "    fid = open(os.path.join(folder, \"id.txt\")) \n",
    "    for a, b, sim, ids in zip(fa, fb, fsim, fid):\n",
    "        qid, _, docid, _, score, _ = ids.replace(\"\\n\", \"\").split()\n",
    "        directory = \"/mnt/collections/w85yang/cosine/topic_doc_mat/{}\".format(qid)\n",
    "        out = os.path.join(directory, \"{}.npy\".format(docid))\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "        a = a.replace(\"\\n\", \"\")\n",
    "        b = b.replace(\"\\n\", \"\")\n",
    "        sim_mat = calculate_cosine_similarity_matrix(a, b)\n",
    "        np.save(out, sim_mat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Generate IDF files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def load_idf():\n",
    "    df = pd.read_csv(\"/mnt/collections/w85yang/wikiextractor/idf_all/idf_terms.csv\")\n",
    "    return df.set_index('token')[\"idf\"].to_dict()\n",
    "\n",
    "def get_qidf(a):\n",
    "    query_terms = tknzr.tokenize(a)\n",
    "    qidf = []\n",
    "    for w in query_terms:\n",
    "        if w in idfs:\n",
    "            qidf.append(idfs[w])\n",
    "        else:\n",
    "            qidf.append(15)\n",
    "    return qidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idfs = load_idf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "qid2idf = {}\n",
    "for year in [\"train_2011\", \"test_2011\", \"train_2013\", \"test_2013\"]:\n",
    "    folder = \"/u4/w85yang/deep-tweet-search/data/twitter/order_by_rel/{}\".format(year)\n",
    "    fa = open(os.path.join(folder, \"a.toks\")) \n",
    "    fb = open(os.path.join(folder, \"b.toks\")) \n",
    "    fsim = open(os.path.join(folder, \"sim.txt\")) \n",
    "    fid = open(os.path.join(folder, \"id.txt\")) \n",
    "    for a, b, sim, ids in zip(fa, fb, fsim, fid):\n",
    "        qid, _, docid, _, score, _ = ids.replace(\"\\n\", \"\").split()\n",
    "        a = a.replace(\"\\n\", \"\")\n",
    "        directory = \"/mnt/collections/w85yang/cosine/query_idf/topic_term_idf\"\n",
    "        out = os.path.join(directory, \"{}.npy\".format(qid))\n",
    "        if qid in qid2idf:\n",
    "            continue\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "        idf = get_qidf(a)\n",
    "        np.save(out, idf)\n",
    "        qid2idf[qid] = idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
