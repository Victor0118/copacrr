{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd \n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model =  KeyedVectors.load_word2vec_format('/Castor-data/embeddings/word2vec/GoogleNews-vectors-negative300.txt', binary=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'is',\n",
       " 'a',\n",
       " 'cooool',\n",
       " '#dummysmiley',\n",
       " ':',\n",
       " ':-)',\n",
       " ':-P',\n",
       " '<3',\n",
       " 'and',\n",
       " 'some',\n",
       " 'arrows',\n",
       " '<',\n",
       " '>',\n",
       " '->',\n",
       " '<--']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "tknzr = TweetTokenizer()\n",
    "s0 = \"This is a cooool #dummysmiley: :-) :-P <3 and some arrows < > -> <--\"\n",
    "tknzr.tokenize(s0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cos_sim(a, b):\n",
    "    return dot(a, b)/(norm(a)*norm(b))\n",
    "\n",
    "def calculate_cosine_similarity_matrix(query, document):\n",
    "    query_terms = tknzr.tokenize(query)\n",
    "    doc_terms = tknzr.tokenize(document)\n",
    "    \n",
    "    sim_mat = []\n",
    "    for query_term in query_terms:\n",
    "        row = []\n",
    "        for doc_term in doc_terms:\n",
    "            if doc_term not in model:\n",
    "                doc_term = \"UNK\"\n",
    "            if query_term not in model:\n",
    "                query_term = \"UNK\"\n",
    "            \n",
    "            sim_vector = cos_sim(model[query_term], model[doc_term])\n",
    "            row.append(sim_vector)\n",
    "        sim_mat.append(row)\n",
    "        \n",
    "    # print(np.array(sim_mat))\n",
    "    return sim_mat\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "46037it [00:42, 1071.10it/s]\n",
      "54986it [00:47, 1167.68it/s]\n",
      "60000it [00:51, 1154.78it/s]\n",
      "55000it [00:57, 955.66it/s] \n"
     ]
    }
   ],
   "source": [
    "#  Generate from the new ql results\n",
    "# for year in range(2011, 2015):\n",
    "#     f = open(\"/u4/w85yang/MatchZoo/data/tweets/TweetCorpus/test_ql_{}.txt\".format(year)) \n",
    "#     for l in tqdm(f):\n",
    "#         sim, a, b, qid, docid = l.replace(\"\\n\", \"\").split(\"\\t\")\n",
    "#         directory = \"/mnt/collections/w85yang/cosine/topic_doc_mat/{}\".format(qid)\n",
    "#         out = os.path.join(directory, \"{}.npy\".format(docid))\n",
    "#         if not os.path.exists(directory):\n",
    "#             os.makedirs(directory)\n",
    "#         sim_mat = calculate_cosine_similarity_matrix(a, b)\n",
    "#         np.save(out, sim_mat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 28s, sys: 8.49 s, total: 2min 36s\n",
      "Wall time: 2min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for year in [\"test_2011\", \"train_2011\", \"train_2013\", \"test_2013\"]:\n",
    "    folder = \"/u4/w85yang/deep-tweet-search/data/twitter/order_by_rel/{}\".format(year)\n",
    "    fa = open(os.path.join(folder, \"a.toks\")) \n",
    "    fb = open(os.path.join(folder, \"b.toks\")) \n",
    "    fsim = open(os.path.join(folder, \"sim.txt\")) \n",
    "    fid = open(os.path.join(folder, \"id.txt\")) \n",
    "    for a, b, sim, ids in zip(fa, fb, fsim, fid):\n",
    "        qid, _, docid, _, score, _ = ids.replace(\"\\n\", \"\").split()\n",
    "        directory = \"/mnt/collections/w85yang/cosine/topic_doc_mat/{}\".format(qid)\n",
    "        out = os.path.join(directory, \"{}.npy\".format(docid))\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "        a = a.replace(\"\\n\", \"\")\n",
    "        b = b.replace(\"\\n\", \"\")\n",
    "        sim_mat = calculate_cosine_similarity_matrix(a, b)\n",
    "        np.save(out, sim_mat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Generate IDF files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def load_idf():\n",
    "    df = pd.read_csv(\"/mnt/collections/w85yang/wikiextractor/idf_all/idf_terms.csv\")\n",
    "    return df.set_index('token')[\"idf\"].to_dict()\n",
    "\n",
    "def get_qidf(a):\n",
    "    query_terms = tknzr.tokenize(a)\n",
    "    qidf = []\n",
    "    for w in query_terms:\n",
    "        if w in idfs:\n",
    "            qidf.append(idfs[w])\n",
    "        else:\n",
    "            qidf.append(15)\n",
    "    return qidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idfs = load_idf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "qid2idf_text = {}\n",
    "for year in [\"train_2011\", \"test_2011\", \"train_2013\", \"test_2013\"]:\n",
    "    folder = \"/u4/w85yang/deep-tweet-search/data/twitter/order_by_rel/{}\".format(year)\n",
    "    fa = open(os.path.join(folder, \"a.toks\")) \n",
    "    fb = open(os.path.join(folder, \"b.toks\")) \n",
    "    fsim = open(os.path.join(folder, \"sim.txt\")) \n",
    "    fid = open(os.path.join(folder, \"id.txt\")) \n",
    "    for a, b, sim, ids in zip(fa, fb, fsim, fid):\n",
    "        qid, _, docid, _, score, _ = ids.replace(\"\\n\", \"\").split()\n",
    "        a = a.replace(\"\\n\", \"\")\n",
    "        directory = \"v\"\n",
    "        out = os.path.join(directory, \"{}.npy\".format(qid))\n",
    "        if qid in qid2idf_text:\n",
    "            continue\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "        idf = get_qidf(a)\n",
    "        np.save(out, idf)\n",
    "        qid2idf_text[qid] = (idf, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([3.5979018369470648,\n",
       "  3.0020393178049183,\n",
       "  9.0048671154949673,\n",
       "  6.195811190833818,\n",
       "  5.3174995951439712],\n",
       " 'australian open djokovic vs murray')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qid2idf_text[\"71\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "68526it [00:00, 685252.48it/s]\u001b[A\n",
      "137830it [00:00, 687569.03it/s]\u001b[A\n",
      "225722it [00:00, 735613.85it/s]\u001b[A\n",
      "262466it [00:00, 798418.77it/s]\u001b[A"
     ]
    }
   ],
   "source": [
    "f = open(\"/u4/w85yang/copacrr/data/qrels.adhoc.6y\", \"w\") # test_ql_2014\n",
    "f2 = open(\"/u4/w85yang/deep-tweet-search/data/twitter/qrels.all.txt\")\n",
    "qid2rel_docids = {}\n",
    "for l in tqdm(f2):\n",
    "    qid, _, docid, sim = l.replace(\"\\n\", \"\").split()\n",
    "    if sim != \"1\" and sim != \"2\":\n",
    "        continue\n",
    "    if qid not in qid2rel_docids:\n",
    "        qid2rel_docids[qid] = set()\n",
    "    qid2rel_docids[qid].add(docid)\n",
    "    \n",
    "for year in [\"train_2011\", \"test_2011\", \"train_2013\", \"test_2013\"]:\n",
    "    folder = \"/u4/w85yang/deep-tweet-search/data/twitter/order_by_rel/{}\".format(year)\n",
    "    fa = open(os.path.join(folder, \"a.toks\")) \n",
    "    fb = open(os.path.join(folder, \"b.toks\")) \n",
    "    fsim = open(os.path.join(folder, \"sim.txt\")) \n",
    "    fid = open(os.path.join(folder, \"id.txt\")) \n",
    "    for a, b, sim, ids in zip(fa, fb, fsim, fid):\n",
    "        qid, _, docid, _, score, _ = ids.replace(\"\\n\", \"\").split()\n",
    "        sim = 0\n",
    "        if qid in qid2rel_docids and docid in qid2rel_docids[qid]:\n",
    "            sim = 1\n",
    "        f.write(\"{} {} {} {}\\n\".format(qid, 0, docid, sim))\n",
    "\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# qid 76 does not has relevant docs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
